---
title: "AI‑Augmented Engineering: Realizing the Value After the Hype"
authorHandle: marcoow
tags: [ai, process, mainmatter]
bio: "Marco Otte-Witte"
description: "…"
tagline: "<p>We’ve made it past the peak of the AI hype cycle. The days of the overblown claims, the panic, and endless thinkpieces in which authors are battling for who can make the boldest claims and thus look the most visionary are over – what we’re left with is more tangible, more boring, but also far more useful: the understanding that AI is a tool that enables real productivity improvements for software engineering teams.</p>"
---

AI, if applied right, is helping teams work faster, better, and more confidently. And while nobody is able to exactly quantify the productivity gain – opinions vary widely between let's say 10% and 10x – the exact number doesn't matter anyway. Even the most defensive estimates result in numbers that must make everyone ask themselves: how can I realize that productivity gain for myself and my team?

This shift creates peer pressure as well. Once some teams can move significantly faster, it forces everyone else to catch up or risk falling behind. An increased level of output for the same input becomes the new baseline, not the exception. That results in new tension as well. Many developers have concerns about code quality, environmental, ethical, or strategic aspects of AI. While those are legitimate concerns, opting out and not leveraging this new capability is asking a lot from any decision maker when the competition is moving ahead and happily increasing their output, and threatening to get ahead.

Yet, successfully leveraging AI in a software engineering context, in particular for mid to large teams and organizations, is not trivial. It’s not just getting everyone a ChatGPT license but a holistic shift in how a team operates. At the same time, there are few established best practices, and everyone is still figuring this out in real time.

### What Gains Are to Be Realized?

While the field continues developing and changing fast, the productivity gains are real and increasingly well-understood. First and most obviously, AI can accelerate code production. What began as smarter autocompletion has quickly evolved into full code and test generation and agentic systems that can build entire features and submodules, commit to git and open pull requests automatically. This doesn’t just shave seconds off keystrokes—it can save hours of an engineer's time.

Applications of AI don't stop at code generation though but extend beyond, e.g. to onboarding. New engineers joining a team often spend days and weeks trying to wrap their heads around a codebase. With AI-powered tools, that process can be compressed as AI can be leveraged to prepare information for easier consumption. System diagrams can be generated on demand, unfamiliar code, design patterns, or component relationships can be explained and summarized in an instant. Searching a codebase no longer relies on precise keyword matches—semantic search is changing large codebases are navigated.

Before any code is written at all, AI also accelerates planning and architecture. Drafting tasks, feature or architecture proposals can be assisted by AI in ways that save time and help teams iterate faster.

In debugging and maintenance, AI is proving valuable as well. When systems misbehave in production, AI tools can assist in analyzing logs, correlating symptoms, and narrowing down possible causes. What might have taken a human hours of digging can now be accelerated with a well-constructed prompt. None of these tools replace expertise, but they accelerate a wide range of tasks significantly.

### Risks and Challenges

That said, it's not all fun and games. AI makes mistakes or hallucinates – often with a disturbing level of confidence. Mistakes range from made-up packages to architectural incoherence or inefficient implementations. This becomes especially problematic in large systems or mature teams, where consistency, clarity, and architecture matter – where someone might go all in on vibe-coding for their toy project they work on on the weekend, that approach has no place in professional software development. You don’t want a model pulling in bits and pieces of different patterns and practices it has seen across the internet and dropping them into your system. The result is neither reliable nor maintainable.

Avoiding that outcome requires strong guardrails. Providing AI tools the context and direction they need to be efficient is the essential first step. But human oversight is equally critical. Developers need to review AI output with the same or greater scrutiny they apply to human-written code. And they need new skills—prompting effectively, knowing where to trust the AI, and where to double-check. Without this human-in-the-loop approach, you really do risk ending up with exactly what the skeptics predict: an unmaintainable pile of garbage with eventually way out.

Another serious challenge is intellectual property and data security. AI models don’t clearly distinguish between inspiration and duplication. They can output snippets that violate licenses or reproduce copyrighted material. At the same time, teams risk leaking their own intellectual property to the AI providers, training the models that might reproduce the same code elsewhere eventually. That could mean giving away competitive secrets, or worse, exposing sensitive user data. It's essential to have in place systems to sanitize prompts, guardrails that prevent sensitive data from leaving secure environments, and the ability to trace and audit AI outputs.

Then there’s the challenge of tooling complexity and fragmentation. The ecosystem is moving fast. There are multiple competing approaches to everything from code generation to test automation. Teams trying to integrate too many tools at once find themselves in a mess of conflicting workflows and overlapping features. Some developers use one tool, others another – context switching increases, confidence in the output decreases.

If code production is in fact accelerated successfully through AI, the surrounding environment might not be able to keep up. More code being written and committed faster only helps if the delivery pipeline is ready for it and can in fact ship all that code to production systems efficiently and reliably. If deployments are slow or even still manual or if testing isn't automated, stable, and comprehensive, accelerated code production won't translate to accelerated value creation. In fact, the opposite: larger, more complex deployments lead to larger and more brittle releases and velocity drops instead of rising.

All that shows that AI only increases the importance of what has always distinguished great engineering teams from the rest: a strong engineering culture and the infrastructure that's based on. The companies that will struggle with adopting AI will be the same ones that have struggled with quality, velocity, and consistency before AI. AI just increases the pressure – if practices are weak, systems brittle, reviews inconsistent, AI will expose and amplify that, making the teams that are already at a competitive disadvantage fall behind further.

### Our New AI Offering

Mainmatter has helped clients build strong engineering cultures and infrastructures for many years. That work is more important than ever, now extended by an additional aspect: helping teams leverage AI for their engineering work in a way that actually accelerates their value creation sustainably.

We’re today launching a new program with which we lead clients through the process of adopting AI for their organization. Here’s what it looks like:

1. First, we assess the current state including team size, experience levels, tooling, development practices, testing and release processes, observability, etc. We want to understand how mature a team is, and where potential blockers for accelerating value creation are.
2. If necessary, we support the client overcome those impediments, e.g. fixing broken or adding missing automation, infrastructure and observability.
3. Once the foundation is set up, we roll out tools incrementally along with the necessary guardrails and mentoring of engineers.

Adapting AI to accelerate a software engineering teams's output isn't just rolling out a new tool. It’s a holistic transformation in how a team operates. AI won’t replace developers. But teams who use AI well will replace teams who don’t. We’re here to help make sure you’re on the winning side of that shift.

<!--
  # Outline

  * status quo
    * moved past peak hype, dust has settled
    * clear that what remains is neither
      * AGI and complete automation of everything
      * huge pile of shit and endless cleanup-after-ai projects
    * instead, substantial productivity increase
      * whatever percentage between 10% and 1000% you want to assume – tbd
      * doesn't matter: 10% is enough so every CTO needs to ask how to get it
    * our understanding is also not anymore CTOs are looking to lay off entire teams – instead of generating the same output with less input, the market will generate more output with the same input
      * software isn't done eating the world, it's just taking larger bites now
      * …and nobody can drop out of the accelerated rate unless they want to be left in the dust
      * the ability to move faster forces everyone to move faster (more output with same input, not same output with less input)
      * there are lots of (valid) concerns that many people still have though and it's important for leadership to take those serious
        (Clear communication from executives that AI tools are meant to augment, not replace developers helps build psychological safety.)
    * yet, not a trivial task, nobody really has experience, particularly not how to apply to larger organizations
  * what gains are to be realized?
    * across the board:
      * accelerate code production obviously – super powerful auto-complete all the way to full agentic coding both code and tests
      * accelerate onboarding by making it easier to reason about a new codebase, e.g. creating diagrams of existing codebases, searching in codebases
      * assist in preparing work and defining architectures
      * DevOps & maintenance: debug issues
  * what are risks and challenges?
    * hallucinations obviously & degraded quality
      * vibe coding fancy term but not what anyone actually does in prod for larger systems/teams
      * this ranges from just bad code to a mix of patterns and architectures
        * strong guardrails required both automated e.g. agents.md as well as human oversight
      * new skills need to be developed (how do I prompt etc.)
      * worst-case scenario is indeed what anit-AI people say: huge pile of shit and no way out
      * -> human in the loop required, also to remain in control of expertise
    * IP and Data security
      * accidental violations of open source licenses or using copyrighted material can happen – important to mitigate the risk
      * also teams' own source code will be leaked to AI – can be critical or not
      * worst case is leaking sensitive (user) data to AI
      * important to build infrastructure that prevents those
    * tooling complexity and fragmentation
      * fast pace of development and fragmentation of tools can lead to huge complexity when you use different tools all the time, constantly need to switch, different people use different tools etc.
    * accelerated code production ends up being problem if no stable delivery pipeline
      * deployments get larger, more complex
      * manual testing ends up being blocker for high velocity
    * -> all of the problems that companies with lack of engineering culture always had
      * strong engineering culture and infrastructure more important than ever
        * if you want to drive a formula 1 engine you need to have the right car, tires, and a race track – otherwise you just crash into a wall
          * (and your competition that has the right equipment drives circles around you)
      * & Mainmatter has worked with clients to improve for many years
  * what's our role in this?
    * what does this mean for an engineering consultancy like mainmatter?
      * are our service no longer needed?
    * quite the opposite: we worked on improving teams for years:
      * better tech, better practices, better infrastructure
      * strong eng culture is more important than ever before
    * this is the next step: enabling teams to leverage AI effectively, sustainably, and reliably
      * companies that don't keep pace will fall behind and be left in the dust
    * what's the offering:
      1. assess the status quo: team size/composition/experience level, dev infrastructure, process practices, testing setup and practices, release and deployment strategy and process, production telemetry and analytics setup
      2. define a goal and roadmap together with the client team; in many cases first steps will be:
        1. ensure dev infrastructure is in place so it's not a blocker: full automated QA, continuous deployment process, telemetry
        2. set up AI tools and guardrails: agents.md, guidance for how to use CLI agents, infrastructure for agents to get feedback from CI etc.; potentially this includes e.g. writing additional MCP servers or CLI tools to expose more information to AI tools
        3. training for the team
  * our new AI offering: …
-->
